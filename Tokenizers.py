def tokenize_sentence(text):
    return text.split(".")

def tokenize_words(text):
    return text.split(" ")
